{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvALNdt1jRmTGRyQ6DfOC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leondotle/research/blob/main/Reinforcement_Learning_for_Optimal_Folding_Patterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install jax jaxlib gymnax flax optax chex tqdm"
      ],
      "metadata": {
        "id": "7ODp_suzPh2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install jax jaxlib gymnax flax optax chex tqdm matplotlib\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from flax.training.train_state import TrainState\n",
        "import gymnax\n",
        "from gymnax.environments import environment, spaces\n",
        "import numpy as np\n",
        "from typing import Tuple, Dict, NamedTuple\n",
        "import chex\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"JAX devices: {jax.devices()}\")\n",
        "\n",
        "# Simplified Environment Info Structure\n",
        "class EnvInfo(NamedTuple):\n",
        "    height: float\n",
        "    area: float\n",
        "    height_error: float\n",
        "    area_error: float\n",
        "    fold_angle: float\n",
        "    crease_angle: float\n",
        "    solved: bool\n",
        "\n",
        "# Streamlined Origami Environment\n",
        "class OrigamiEnv(environment.Environment):\n",
        "    def __init__(self, target_height=1.0, target_area=15.0):\n",
        "        super().__init__()\n",
        "        self.target_height = target_height\n",
        "        self.target_area = target_area\n",
        "\n",
        "        # Grid parameters for origami geometry\n",
        "        self.a, self.b, self.m, self.n = 1.0, 1.5, 8, 6\n",
        "        i, j = jnp.arange(self.m + 1), jnp.arange(self.n + 1)\n",
        "        self.I_grid, self.J_grid = jnp.meshgrid(i, j, indexing=\"ij\")\n",
        "\n",
        "    @property\n",
        "    def default_state(self) -> chex.Array:\n",
        "        return jnp.array([jnp.pi/4, jnp.pi/4], dtype=jnp.float32)\n",
        "\n",
        "    def step_env(self, key: chex.PRNGKey, state: chex.Array, action: chex.Array) -> Tuple[chex.Array, float, bool, EnvInfo]:\n",
        "        # Clip actions to valid range\n",
        "        action = jnp.clip(action, 0.01, jnp.pi/2)\n",
        "\n",
        "        # Compute geometry\n",
        "        w, l, h = self._compute_geometry(action[0], action[1])\n",
        "        area = w * l\n",
        "\n",
        "        # Compute reward\n",
        "        height_error = jnp.abs(h - self.target_height)\n",
        "        area_error = jnp.abs(area - self.target_area)\n",
        "\n",
        "        reward = (1.0 / (1.0 + 2.0 * height_error) +\n",
        "                 1.0 / (1.0 + 0.1 * area_error) -\n",
        "                 0.01 * jnp.sum(jnp.square(action - state)))\n",
        "\n",
        "        done = jnp.logical_and(height_error < 0.2, area_error < 2.0)\n",
        "        reward = jnp.where(done, reward + 3.0, reward)\n",
        "\n",
        "        info = EnvInfo(\n",
        "            height=h, area=area, height_error=height_error, area_error=area_error,\n",
        "            fold_angle=action[0], crease_angle=action[1], solved=done\n",
        "        )\n",
        "\n",
        "        return action, reward, done, info\n",
        "\n",
        "    def reset_env(self, key: chex.PRNGKey) -> Tuple[chex.Array, Dict]:\n",
        "        noise = jax.random.uniform(key, (2,), minval=-0.1, maxval=0.1)\n",
        "        state = jnp.clip(self.default_state + noise, 0.01, jnp.pi/2)\n",
        "        w, l, h = self._compute_geometry(state[0], state[1])\n",
        "        return state, {\"initial_height\": h, \"initial_area\": w * l}\n",
        "\n",
        "    def _compute_geometry(self, fold_angle, crease_angle):\n",
        "        sin_a, cos_a = jnp.sin(crease_angle), jnp.cos(crease_angle)\n",
        "        sin_t = jnp.sin(fold_angle)\n",
        "\n",
        "        gamma = jnp.arcsin(jnp.clip(sin_a * sin_t, -0.999, 0.999))\n",
        "\n",
        "        x = self.I_grid * self.a * jnp.cos(gamma) + (self.J_grid % 2) * self.b * cos_a\n",
        "        y = self.J_grid * self.b * sin_a\n",
        "        z = (self.I_grid % 2) * self.a * sin_a * sin_t\n",
        "\n",
        "        verts = jnp.stack([x.ravel(), y.ravel(), z.ravel()], axis=-1)\n",
        "        return (jnp.maximum(jnp.ptp(verts[:, 0]), 0.001),\n",
        "                jnp.maximum(jnp.ptp(verts[:, 1]), 0.001),\n",
        "                jnp.maximum(jnp.ptp(verts[:, 2]), 0.0))\n",
        "\n",
        "    @property\n",
        "    def name(self) -> str: return \"Origami-v1\"\n",
        "    @property\n",
        "    def num_actions(self) -> int: return 2\n",
        "\n",
        "    def action_space(self) -> spaces.Box:\n",
        "        return spaces.Box(low=jnp.array([0.01, 0.01]), high=jnp.array([jnp.pi/2, jnp.pi/2]), shape=(2,))\n",
        "\n",
        "    def observation_space(self) -> spaces.Box:\n",
        "        return self.action_space()\n",
        "\n",
        "# Simplified Actor-Critic Network\n",
        "class ActorCritic(nn.Module):\n",
        "    action_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(128)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(64)(x)\n",
        "        x = nn.LayerNorm()(x)\n",
        "        x = nn.relu(x)\n",
        "\n",
        "        # Policy head\n",
        "        mean = nn.sigmoid(nn.Dense(self.action_dim)(x)) * jnp.pi/2\n",
        "        log_std = self.param('log_std', nn.initializers.constant(-1.0), (self.action_dim,))\n",
        "\n",
        "        # Value head\n",
        "        value = nn.Dense(1)(x).squeeze(-1)\n",
        "\n",
        "        return mean, log_std, value\n",
        "\n",
        "# Vectorized Environment Wrapper\n",
        "class VectorizedEnv:\n",
        "    def __init__(self, env, num_envs):\n",
        "        self.env = env\n",
        "        self.num_envs = num_envs\n",
        "        self.reset_fn = jax.vmap(env.reset_env)\n",
        "        self.step_fn = jax.vmap(env.step_env)\n",
        "\n",
        "    def reset(self, key):\n",
        "        keys = jax.random.split(key, self.num_envs)\n",
        "        return self.reset_fn(keys)\n",
        "\n",
        "    def step(self, key, states, actions):\n",
        "        keys = jax.random.split(key, self.num_envs)\n",
        "        return self.step_fn(keys, states, actions)\n",
        "\n",
        "# PPO Helper Functions\n",
        "def gaussian_log_prob(x, mean, log_std):\n",
        "    \"\"\"Calculate log probability of multivariate Gaussian with diagonal covariance.\"\"\"\n",
        "    std = jnp.exp(log_std)\n",
        "    return -0.5 * jnp.sum(\n",
        "        jnp.square((x - mean) / std) + 2 * log_std + jnp.log(2 * jnp.pi),\n",
        "        axis=-1\n",
        "    )\n",
        "\n",
        "def ppo_clipped_loss(new_log_probs, old_log_probs, advantages, clip_param=0.2):\n",
        "    \"\"\"PPO clipped surrogate loss.\"\"\"\n",
        "    ratio = jnp.exp(new_log_probs - old_log_probs)\n",
        "    clipped_ratio = jnp.clip(ratio, 1 - clip_param, 1 + clip_param)\n",
        "    surrogate_loss = jnp.minimum(ratio * advantages, clipped_ratio * advantages)\n",
        "    return -jnp.mean(surrogate_loss)\n",
        "\n",
        "def compute_gae(rewards, values, next_values, dones, gamma=0.99, lam=0.95):\n",
        "    \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
        "    advantages = []\n",
        "    gae = 0.0\n",
        "\n",
        "    for t in reversed(range(rewards.shape[0])):\n",
        "        if t == rewards.shape[0] - 1:\n",
        "            next_non_terminal = 1.0 - dones[t]\n",
        "            next_value = next_values\n",
        "        else:\n",
        "            next_non_terminal = 1.0 - dones[t]\n",
        "            next_value = values[t + 1]\n",
        "\n",
        "        delta = rewards[t] + gamma * next_value * next_non_terminal - values[t]\n",
        "        gae = delta + gamma * lam * next_non_terminal * gae\n",
        "        advantages.insert(0, gae)\n",
        "\n",
        "    advantages = jnp.array(advantages)\n",
        "    returns = advantages + values\n",
        "\n",
        "    # Normalize advantages\n",
        "    advantages = (advantages - jnp.mean(advantages)) / (jnp.std(advantages) + 1e-8)\n",
        "\n",
        "    return advantages, returns\n",
        "\n",
        "# Streamlined PPO Training\n",
        "def train_ppo(env, num_envs=32, total_steps=50000, lr=2e-4):\n",
        "    # Initialize\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    model = ActorCritic(action_dim=2)\n",
        "\n",
        "    # Create train state\n",
        "    dummy_obs = jnp.zeros((2,))\n",
        "    params = model.init(key, dummy_obs)\n",
        "\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(1.0),\n",
        "        optax.adam(lr)\n",
        "    )\n",
        "\n",
        "    train_state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
        "\n",
        "    # Create vectorized environment\n",
        "    venv = VectorizedEnv(env, num_envs)\n",
        "\n",
        "    # Training parameters\n",
        "    rollout_len = 64\n",
        "    num_updates = total_steps // (num_envs * rollout_len)\n",
        "\n",
        "    # Action sampling function\n",
        "    @jax.jit\n",
        "    def sample_actions(params, obs, key):\n",
        "        mean, log_std, values = jax.vmap(model.apply, (None, 0))(params, obs)\n",
        "        std = jnp.exp(log_std)\n",
        "\n",
        "        actions = mean + jax.random.normal(key, mean.shape) * std\n",
        "        actions = jnp.clip(actions, 0.01, jnp.pi/2)\n",
        "\n",
        "        log_probs = gaussian_log_prob(actions, mean, log_std)\n",
        "\n",
        "        return actions, log_probs, values\n",
        "\n",
        "    # PPO loss function\n",
        "    def ppo_loss(params, obs, actions, old_log_probs, advantages, returns):\n",
        "        mean, log_std, values = jax.vmap(model.apply, (None, 0))(params, obs)\n",
        "\n",
        "        # Calculate new log probabilities\n",
        "        log_probs = gaussian_log_prob(actions, mean, log_std)\n",
        "\n",
        "        # PPO clipped surrogate loss\n",
        "        policy_loss = ppo_clipped_loss(log_probs, old_log_probs, advantages)\n",
        "\n",
        "        # Value loss (MSE)\n",
        "        value_loss = 0.5 * jnp.mean(jnp.square(values - returns))\n",
        "\n",
        "        # Entropy bonus\n",
        "        entropy = jnp.mean(jnp.sum(log_std + 0.5 * jnp.log(2 * jnp.pi * jnp.e), axis=-1))\n",
        "\n",
        "        total_loss = policy_loss + value_loss - 0.01 * entropy\n",
        "\n",
        "        return total_loss, {\n",
        "            'policy_loss': policy_loss,\n",
        "            'value_loss': value_loss,\n",
        "            'entropy': entropy\n",
        "        }\n",
        "\n",
        "    # Update function\n",
        "    @jax.jit\n",
        "    def update(train_state, batch):\n",
        "        grad_fn = jax.value_and_grad(ppo_loss, has_aux=True)\n",
        "        (loss, info), grads = grad_fn(train_state.params, *batch)\n",
        "        return train_state.apply_gradients(grads=grads), loss, info\n",
        "\n",
        "    # Training loop\n",
        "    key, reset_key = jax.random.split(key)\n",
        "    obs, _ = venv.reset(reset_key)\n",
        "\n",
        "    metrics = {'rewards': [], 'losses': [], 'heights': [], 'areas': []}\n",
        "\n",
        "    for update_idx in tqdm(range(num_updates)):\n",
        "        # Collect rollout\n",
        "        rollout_obs, rollout_actions, rollout_rewards = [], [], []\n",
        "        rollout_log_probs, rollout_values, rollout_dones = [], [], []\n",
        "        rollout_heights, rollout_areas = [], []\n",
        "\n",
        "        for step in range(rollout_len):\n",
        "            key, action_key, step_key = jax.random.split(key, 3)\n",
        "\n",
        "            actions, log_probs, values = sample_actions(train_state.params, obs, action_key)\n",
        "            next_obs, rewards, dones, infos = venv.step(step_key, obs, actions)\n",
        "\n",
        "            # Store rollout data\n",
        "            rollout_obs.append(obs)\n",
        "            rollout_actions.append(actions)\n",
        "            rollout_rewards.append(rewards)\n",
        "            rollout_log_probs.append(log_probs)\n",
        "            rollout_values.append(values)\n",
        "            rollout_dones.append(dones)\n",
        "            rollout_heights.append(infos.height)\n",
        "            rollout_areas.append(infos.area)\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "            # Reset done environments\n",
        "            if jnp.any(dones):\n",
        "                key, reset_key = jax.random.split(key)\n",
        "                reset_obs, _ = venv.reset(reset_key)\n",
        "                obs = jnp.where(dones[:, None], reset_obs, obs)\n",
        "\n",
        "        # Convert to arrays\n",
        "        rollout_obs = jnp.array(rollout_obs)\n",
        "        rollout_actions = jnp.array(rollout_actions)\n",
        "        rollout_rewards = jnp.array(rollout_rewards)\n",
        "        rollout_log_probs = jnp.array(rollout_log_probs)\n",
        "        rollout_values = jnp.array(rollout_values)\n",
        "        rollout_dones = jnp.array(rollout_dones)\n",
        "\n",
        "        # Get final values for GAE\n",
        "        _, _, next_values = jax.vmap(model.apply, (None, 0))(train_state.params, obs)\n",
        "\n",
        "        # Compute advantages using GAE\n",
        "        advantages, returns = compute_gae(rollout_rewards, rollout_values, next_values, rollout_dones)\n",
        "\n",
        "        # Flatten for batch processing\n",
        "        batch_size = rollout_len * num_envs\n",
        "        batch_obs = rollout_obs.reshape(batch_size, -1)\n",
        "        batch_actions = rollout_actions.reshape(batch_size, -1)\n",
        "        batch_log_probs = rollout_log_probs.reshape(batch_size)\n",
        "        batch_advantages = advantages.reshape(batch_size)\n",
        "        batch_returns = returns.reshape(batch_size)\n",
        "\n",
        "        # Multiple PPO updates\n",
        "        for _ in range(4):\n",
        "            train_state, loss, info = update(\n",
        "                train_state,\n",
        "                (batch_obs, batch_actions, batch_log_probs, batch_advantages, batch_returns)\n",
        "            )\n",
        "\n",
        "        # Record metrics\n",
        "        metrics['rewards'].append(float(jnp.mean(rollout_rewards)))\n",
        "        metrics['losses'].append(float(loss))\n",
        "        metrics['heights'].append(float(jnp.mean(jnp.array(rollout_heights))))\n",
        "        metrics['areas'].append(float(jnp.mean(jnp.array(rollout_areas))))\n",
        "\n",
        "        if update_idx % 10 == 0:\n",
        "            print(f\"Update {update_idx}: Reward={metrics['rewards'][-1]:.3f}, \"\n",
        "                  f\"Height={metrics['heights'][-1]:.3f}, Area={metrics['areas'][-1]:.3f}\")\n",
        "\n",
        "    return train_state, metrics\n",
        "\n",
        "# Simplified testing function\n",
        "def test_agent(env, train_state, num_episodes=5):\n",
        "    key = jax.random.PRNGKey(0)\n",
        "    best_reward = -float('inf')\n",
        "    best_config = None\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        key, reset_key = jax.random.split(key)\n",
        "        obs, _ = env.reset_env(reset_key)\n",
        "        episode_reward = 0\n",
        "        step_count = 0\n",
        "\n",
        "        while step_count < 50:  # Max steps per episode\n",
        "            mean, _, _ = train_state.apply_fn(train_state.params, obs)\n",
        "            action = jnp.clip(mean, 0.01, jnp.pi/2)\n",
        "\n",
        "            key, step_key = jax.random.split(key)\n",
        "            obs, reward, done, info = env.step_env(step_key, obs, action)\n",
        "            episode_reward += reward\n",
        "            step_count += 1\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            best_config = {\n",
        "                'fold_angle': float(info.fold_angle),\n",
        "                'crease_angle': float(info.crease_angle),\n",
        "                'height': float(info.height),\n",
        "                'area': float(info.area),\n",
        "                'reward': float(episode_reward)\n",
        "            }\n",
        "\n",
        "        print(f\"Episode {episode+1}: Reward={episode_reward:.3f}, \"\n",
        "              f\"Height={float(info.height):.3f}, Area={float(info.area):.3f}\")\n",
        "\n",
        "    return best_config\n",
        "\n",
        "# Simplified plotting\n",
        "def plot_metrics(metrics):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "    axes[0, 0].plot(metrics['rewards'])\n",
        "    axes[0, 0].set_title('Rewards')\n",
        "    axes[0, 0].set_xlabel('Update')\n",
        "    axes[0, 0].set_ylabel('Average Reward')\n",
        "\n",
        "    axes[0, 1].plot(metrics['losses'])\n",
        "    axes[0, 1].set_title('Losses')\n",
        "    axes[0, 1].set_xlabel('Update')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "\n",
        "    axes[1, 0].plot(metrics['heights'])\n",
        "    axes[1, 0].axhline(y=1.0, color='r', linestyle='--', label='Target')\n",
        "    axes[1, 0].set_title('Heights')\n",
        "    axes[1, 0].set_xlabel('Update')\n",
        "    axes[1, 0].set_ylabel('Height')\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "    axes[1, 1].plot(metrics['areas'])\n",
        "    axes[1, 1].axhline(y=15.0, color='r', linestyle='--', label='Target')\n",
        "    axes[1, 1].set_title('Areas')\n",
        "    axes[1, 1].set_xlabel('Update')\n",
        "    axes[1, 1].set_ylabel('Area')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create environment and train\n",
        "    env = OrigamiEnv(target_height=1.0, target_area=15.0)\n",
        "    print(\"Training agent...\")\n",
        "    train_state, metrics = train_ppo(env, num_envs=32, total_steps=25000)\n",
        "\n",
        "    # Plot results\n",
        "    plot_metrics(metrics)\n",
        "\n",
        "    # Test agent\n",
        "    print(\"\\nTesting agent:\")\n",
        "    best_config = test_agent(env, train_state)\n",
        "\n",
        "    print(f\"\\nBest configuration:\")\n",
        "    for key, value in best_config.items():\n",
        "        print(f\"{key}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "QH1HBd17W7F_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}